{"metadata": {"kernelspec": {"name": "python310", "display_name": "Python 3.10 with Spark", "language": "python3"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "bucket_name = 'iceberg-data-bucket'\ncatalog_name = 'iceberg_data_catalog'\nschema_name = 'demodb_snappy_v1'\nendpoint = \"https://s3.jp-tok.cloud-object-storage.appdomain.cloud\"\naccess_key = \"9d05516331d44f38ba2c230ff036e8a0\"\nsecret_key = \"423c929fb85ff48809011f3264e10e7a87240150a5896b01\"\nraw_data_bucket_name = \"raw-data-bucket\"\ncpd_user=\"cpadmin\"\ncpd_password=\"RkTyzx2Dgg6lw0HPfdwBjeiofq1sai8w\"", "metadata": {"msg_id": "75110d11-05ca-4587-80dd-034b7618521a"}, "outputs": [], "execution_count": 35}, {"cell_type": "code", "source": "conf=spark.sparkContext.getConf()\nspark.stop()\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_utc_timestamp\nimport base64,getpass\nimport pandas as pd\nimport io\n\nwxd_username=\"cpadmin\"\nwxd_hms_username=\"ibmlhapikey_\"+wxd_username\n# wxd_hms_password=\"RkTyzx2Dgg6lw0HPfdwBjeiofq1sai8w\"\nwxd_hms_password=\"aW5zdGFuY2U6OnFrMjk2Mm9rdzVsOTc1d2Q=\"\nstring_to_encode=wxd_username+\":\"+wxd_hms_password\nwxd_encoded_apikey=\"ZenApiKey \"+base64.b64encode(string_to_encode.encode(\"utf-8\")).decode(\"utf-8\")\n\nconf.setAll([(\"spark.hive.metastore.client.plain.username\", wxd_hms_username), \\\n    (\"spark.hive.metastore.client.plain.password\", wxd_hms_password), \\\n    ('spark.hadoop.hive.wxd.user.name', wxd_username), \\\n    (\"spark.hadoop.wxd.apikey\", wxd_encoded_apikey), \\\n    (\"spark.sql.parquet.compression.codec\", \"snappy\"), \\\n    (\"spark.sql.shuffle.partitions\", \"200\"), \\\n    (f\"spark.hadoop.fs.s3a.bucket.{raw_data_bucket_name}.endpoint\", endpoint), \\\n    (f\"spark.hadoop.fs.s3a.bucket.{raw_data_bucket_name}.access.key\", access_key), \\\n    (f\"spark.hadoop.fs.s3a.bucket.{raw_data_bucket_name}.secret.key\", secret_key)\n])\n\nspark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()", "metadata": {"msg_id": "b78b56b9-841b-4fa2-afa1-4c0b9292938f"}, "outputs": [], "execution_count": 36}, {"cell_type": "code", "source": "spark", "metadata": {"msg_id": "099e1a4f-d071-46fd-b9af-6e9a9d695e7b"}, "outputs": [{"execution_count": 37, "output_type": "execute_result", "data": {"text/plain": "<pyspark.sql.session.SparkSession at 0x7fd21ad4e830>", "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://spark-master-headless-b301a6b3-3f01-47b1-b3df-7dfca696c3c5:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.4.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://spark-master-headless-b301a6b3-3f01-47b1-b3df-7dfca696c3c5:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>python3.10</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "}, "metadata": {}}], "execution_count": 37}, {"cell_type": "code", "source": "# for key, value in spark.sparkContext.getConf().getAll():\n#     print(f\"{key} = {value}\")", "metadata": {"msg_id": "3da75788-0352-4df1-9174-6cbb52287bec"}, "outputs": [], "execution_count": 38}, {"cell_type": "code", "source": "spark.sql(f\"show databases from {catalog_name}\").show()", "metadata": {"msg_id": "e1047412-3e0a-478c-9db9-dc459e9b2d58"}, "outputs": [{"name": "stdout", "text": "+--------------------+\n|           namespace|\n+--------------------+\n|             default|\n|                 kym|\n|                test|\n|              demodb|\n|               stage|\n|wxd_system_data_d...|\n|wxd_system_data_d...|\n|       demodb_snappy|\n|    demodb_snappy_v1|\n+--------------------+\n\n", "output_type": "stream"}], "execution_count": 39}, {"cell_type": "code", "source": "spark.sql(f\"create database if not exists {catalog_name}.{schema_name} LOCATION 's3a://{bucket_name}/{schema_name}/'\")", "metadata": {"msg_id": "07b0b694-b7a2-4730-99d0-5cf04a553a24"}, "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "DataFrame[]"}, "metadata": {}}], "execution_count": 8}, {"cell_type": "code", "source": "spark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.dbgen_version (\n    dv_version                STRING,\n    dv_create_date            DATE,\n    dv_create_time            STRING,\n    dv_cmdline_args           STRING\n)\nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create customer_address table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.customer_address (\n    ca_address_sk             integer,\n    ca_address_id             varchar(16),\n    ca_street_number          varchar(10),\n    ca_street_name            varchar(60),\n    ca_street_type            varchar(15),\n    ca_suite_number           varchar(10),\n    ca_city                   varchar(60),\n    ca_county                 varchar(30),\n    ca_state                  varchar(2),\n    ca_zip                    varchar(10),\n    ca_country                varchar(20),\n    ca_gmt_offset             decimal(5,2),\n    ca_location_type          varchar(20)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create customer_demographics table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.customer_demographics (\n    cd_demo_sk                integer,\n    cd_gender                 varchar(1),\n    cd_marital_status         varchar(1),\n    cd_education_status       varchar(20),\n    cd_purchase_estimate      integer,\n    cd_credit_rating          varchar(10),\n    cd_dep_count              integer,\n    cd_dep_employed_count     integer,\n    cd_dep_college_count      integer\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create date_dim table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.date_dim (\n    d_date_sk                 integer,\n    d_date_id                 varchar(16),\n    d_date                    date,\n    d_month_seq               integer,\n    d_week_seq                integer,\n    d_quarter_seq             integer,\n    d_year                    integer,\n    d_dow                     integer,\n    d_moy                     integer,\n    d_dom                     integer,\n    d_qoy                     integer,\n    d_fy_year                 integer,\n    d_fy_quarter_seq          integer,\n    d_fy_week_seq             integer,\n    d_day_name                varchar(9),\n    d_quarter_name            varchar(6),\n    d_holiday                 varchar(1),\n    d_weekend                 varchar(1),\n    d_following_holiday       varchar(1),\n    d_first_dom               integer,\n    d_last_dom                integer,\n    d_same_day_ly             integer,\n    d_same_day_lq             integer,\n    d_current_day             varchar(1),\n    d_current_week            varchar(1),\n    d_current_month           varchar(1),\n    d_current_quarter         varchar(1),\n    d_current_year            varchar(1)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create warehouse table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.warehouse (\n    w_warehouse_sk            integer,\n    w_warehouse_id            varchar(16),\n    w_warehouse_name          varchar(20),\n    w_warehouse_sq_ft         integer,\n    w_street_number           varchar(10),\n    w_street_name             varchar(60),\n    w_street_type             varchar(15),\n    w_suite_number            varchar(10),\n    w_city                    varchar(60),\n    w_county                  varchar(30),\n    w_state                   varchar(2),\n    w_zip                     varchar(10),\n    w_country                 varchar(20),\n    w_gmt_offset              decimal(5,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create ship_mode table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.ship_mode (\n    sm_ship_mode_sk           integer,\n    sm_ship_mode_id           varchar(16),\n    sm_type                   varchar(30),\n    sm_code                   varchar(10),\n    sm_carrier                varchar(20),\n    sm_contract               varchar(20)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create time_dim table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.time_dim (\n    t_time_sk                 integer,\n    t_time_id                 varchar(16),\n    t_time                    integer,\n    t_hour                    integer,\n    t_minute                  integer,\n    t_second                  integer,\n    t_am_pm                   varchar(2),\n    t_shift                   varchar(20),\n    t_sub_shift               varchar(20),\n    t_meal_time               varchar(20)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create reason table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.reason (\n    r_reason_sk               integer,\n    r_reason_id               varchar(16),\n    r_reason_desc             varchar(100)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create income_band table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.income_band (\n    ib_income_band_sk         integer,\n    ib_lower_bound            integer,\n    ib_upper_bound            integer\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create item table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.item (\n    i_item_sk                 integer,\n    i_item_id                 varchar(16),\n    i_rec_start_date          date,\n    i_rec_end_date            date,\n    i_item_desc               varchar(200),\n    i_current_price           decimal(7,2),\n    i_wholesale_cost          decimal(7,2),\n    i_brand_id                integer,\n    i_brand                   varchar(50),\n    i_class_id                integer,\n    i_class                   varchar(50),\n    i_category_id             integer,\n    i_category                varchar(50),\n    i_manufact_id             integer,\n    i_manufact                varchar(50),\n    i_size                    varchar(20),\n    i_formulation             varchar(20),\n    i_color                   varchar(20),\n    i_units                   varchar(10),\n    i_container               varchar(10),\n    i_manager_id              integer,\n    i_product_name            varchar(50)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create store table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.store (\n    s_store_sk                integer,\n    s_store_id                varchar(16),\n    s_rec_start_date          date,\n    s_rec_end_date            date,\n    s_closed_date_sk          integer,\n    s_store_name              varchar(50),\n    s_number_employees        integer,\n    s_floor_space             integer,\n    s_hours                   varchar(20),\n    s_manager                 varchar(40),\n    s_market_id               integer,\n    s_geography_class         varchar(100),\n    s_market_desc             varchar(100),\n    s_market_manager          varchar(40),\n    s_division_id             integer,\n    s_division_name           varchar(50),\n    s_company_id              integer,\n    s_company_name            varchar(50),\n    s_street_number           varchar(10),\n    s_street_name             varchar(60),\n    s_street_type             varchar(15),\n    s_suite_number            varchar(10),\n    s_city                    varchar(60),\n    s_county                  varchar(30),\n    s_state                   varchar(2),\n    s_zip                     varchar(10),\n    s_country                 varchar(20),\n    s_gmt_offset              decimal(5,2),\n    s_tax_percentage          decimal(5,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create call_center table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.call_center (\n    cc_call_center_sk         integer,\n    cc_call_center_id         varchar(16),\n    cc_rec_start_date         date,\n    cc_rec_end_date           date,\n    cc_closed_date_sk         integer,\n    cc_open_date_sk           integer,\n    cc_name                   varchar(50),\n    cc_class                  varchar(50),\n    cc_employees              integer,\n    cc_sq_ft                  integer,\n    cc_hours                  varchar(20),\n    cc_manager                varchar(40),\n    cc_mkt_id                 integer,\n    cc_mkt_class              varchar(50),\n    cc_mkt_desc               varchar(100),\n    cc_market_manager         varchar(40),\n    cc_division               integer,\n    cc_division_name          varchar(50),\n    cc_company                integer,\n    cc_company_name           varchar(50),\n    cc_street_number          varchar(10),\n    cc_street_name            varchar(60),\n    cc_street_type            varchar(15),\n    cc_suite_number           varchar(10),\n    cc_city                   varchar(60),\n    cc_county                 varchar(30),\n    cc_state                  varchar(2),\n    cc_zip                    varchar(10),\n    cc_country                varchar(20),\n    cc_gmt_offset             decimal(5,2),\n    cc_tax_percentage         decimal(5,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create customer table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.customer (\n    c_customer_sk             integer,\n    c_customer_id             varchar(16),\n    c_current_cdemo_sk        integer,\n    c_current_hdemo_sk        integer,\n    c_current_addr_sk         integer,\n    c_first_shipto_date_sk    integer,\n    c_first_sales_date_sk     integer,\n    c_salutation              varchar(10),\n    c_first_name              varchar(20),\n    c_last_name               varchar(30),\n    c_preferred_cust_flag     varchar(1),\n    c_birth_day               integer,\n    c_birth_month             integer,\n    c_birth_year              integer,\n    c_birth_country           varchar(20),\n    c_login                   varchar(13),\n    c_email_address           varchar(50),\n    c_last_review_date        varchar(10)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create web_site table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.web_site (\n    web_site_sk               integer,\n    web_site_id               varchar(16),\n    web_rec_start_date        date,\n    web_rec_end_date          date,\n    web_name                  varchar(50),\n    web_open_date_sk          integer,\n    web_close_date_sk         integer,\n    web_class                 varchar(50),\n    web_manager               varchar(40),\n    web_mkt_id                integer,\n    web_mkt_class             varchar(50),\n    web_mkt_desc              varchar(100),\n    web_market_manager        varchar(40),\n    web_company_id            integer,\n    web_company_name          varchar(50),\n    web_street_number         varchar(10),\n    web_street_name           varchar(60),\n    web_street_type           varchar(15),\n    web_suite_number          varchar(10),\n    web_city                  varchar(60),\n    web_county                varchar(30),\n    web_state                 varchar(2),\n    web_zip                   varchar(10),\n    web_country               varchar(20),\n    web_gmt_offset            decimal(5,2),\n    web_tax_percentage        decimal(5,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create store_returns table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.store_returns (\n    sr_returned_date_sk       integer,\n    sr_return_time_sk         integer,\n    sr_item_sk                integer,\n    sr_customer_sk            integer,\n    sr_cdemo_sk               integer,\n    sr_hdemo_sk               integer,\n    sr_addr_sk                integer,\n    sr_store_sk               integer,\n    sr_reason_sk              integer,\n    sr_ticket_number          integer,\n    sr_return_quantity        integer,\n    sr_return_amt             decimal(7,2),\n    sr_return_tax             decimal(7,2),\n    sr_return_amt_inc_tax     decimal(7,2),\n    sr_fee                    decimal(7,2),\n    sr_return_ship_cost       decimal(7,2),\n    sr_refunded_cash          decimal(7,2),\n    sr_reversed_charge        decimal(7,2),\n    sr_store_credit           decimal(7,2),\n    sr_net_loss               decimal(7,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create household_demographics table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.household_demographics (\n    hd_demo_sk                integer,\n    hd_income_band_sk         integer,\n    hd_buy_potential          varchar(15),\n    hd_dep_count              integer,\n    hd_vehicle_count          integer\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create web_page table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.web_page (\n    wp_web_page_sk            integer,\n    wp_web_page_id            varchar(16),\n    wp_rec_start_date         date,\n    wp_rec_end_date           date,\n    wp_creation_date_sk       integer,\n    wp_access_date_sk         integer,\n    wp_autogen_flag           varchar(1),\n    wp_customer_sk            integer,\n    wp_url                    varchar(100),\n    wp_type                   varchar(50),\n    wp_char_count             integer,\n    wp_link_count             integer,\n    wp_image_count            integer,\n    wp_max_ad_count           integer\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create promotion table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.promotion (\n    p_promo_sk                integer,\n    p_promo_id                varchar(16),\n    p_start_date_sk           integer,\n    p_end_date_sk             integer,\n    p_item_sk                 integer,\n    p_cost                    decimal(15,2),\n    p_response_target         integer,\n    p_promo_name              varchar(50),\n    p_channel_dmail           varchar(1),\n    p_channel_email           varchar(1),\n    p_channel_catalog         varchar(1),\n    p_channel_tv              varchar(1),\n    p_channel_radio           varchar(1),\n    p_channel_press           varchar(1),\n    p_channel_event           varchar(1),\n    p_channel_demo            varchar(1),\n    p_channel_details         varchar(100),\n    p_purpose                 varchar(15),\n    p_discount_active         varchar(1)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create catalog_page table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.catalog_page (\n    cp_catalog_page_sk        integer,\n    cp_catalog_page_id        varchar(16),\n    cp_start_date_sk          integer,\n    cp_end_date_sk            integer,\n    cp_department             varchar(50),\n    cp_catalog_number         integer,\n    cp_catalog_page_number    integer,\n    cp_description            varchar(100),\n    cp_type                   varchar(100)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create inventory table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.inventory (\n    inv_date_sk               integer,\n    inv_item_sk               integer,\n    inv_warehouse_sk          integer,\n    inv_quantity_on_hand      integer\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create catalog_returns table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.catalog_returns (\n    cr_returned_date_sk       integer,\n    cr_returned_time_sk       integer,\n    cr_item_sk                integer,\n    cr_refunded_customer_sk   integer,\n    cr_refunded_cdemo_sk      integer,\n    cr_refunded_hdemo_sk      integer,\n    cr_refunded_addr_sk       integer,\n    cr_returning_customer_sk  integer,\n    cr_returning_cdemo_sk     integer,\n    cr_returning_hdemo_sk     integer,\n    cr_returning_addr_sk      integer,\n    cr_call_center_sk         integer,\n    cr_catalog_page_sk        integer,\n    cr_ship_mode_sk           integer,\n    cr_warehouse_sk           integer,\n    cr_reason_sk              integer,\n    cr_order_number           integer,\n    cr_return_quantity        integer,\n    cr_return_amount          decimal(7,2),\n    cr_return_tax             decimal(7,2),\n    cr_return_amt_inc_tax     decimal(7,2),\n    cr_fee                    decimal(7,2),\n    cr_return_ship_cost       decimal(7,2),\n    cr_refunded_cash          decimal(7,2),\n    cr_reversed_charge        decimal(7,2),\n    cr_store_credit           decimal(7,2),\n    cr_net_loss               decimal(7,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create web_returns table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.web_returns (\n    wr_returned_date_sk       integer,\n    wr_returned_time_sk       integer,\n    wr_item_sk                integer,\n    wr_refunded_customer_sk   integer,\n    wr_refunded_cdemo_sk      integer,\n    wr_refunded_hdemo_sk      integer,\n    wr_refunded_addr_sk       integer,\n    wr_returning_customer_sk  integer,\n    wr_returning_cdemo_sk     integer,\n    wr_returning_hdemo_sk     integer,\n    wr_returning_addr_sk      integer,\n    wr_web_page_sk            integer,\n    wr_reason_sk              integer,\n    wr_order_number           integer,\n    wr_return_quantity        integer,\n    wr_return_amt             decimal(7,2),\n    wr_return_tax             decimal(7,2),\n    wr_return_amt_inc_tax     decimal(7,2),\n    wr_fee                    decimal(7,2),\n    wr_return_ship_cost       decimal(7,2),\n    wr_refunded_cash          decimal(7,2),\n    wr_reversed_charge        decimal(7,2),\n    wr_account_credit         decimal(7,2),\n    wr_net_loss               decimal(7,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create web_sales table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.web_sales (\n    ws_sold_date_sk           integer,\n    ws_sold_time_sk           integer,\n    ws_ship_date_sk           integer,\n    ws_item_sk                integer,\n    ws_bill_customer_sk       integer,\n    ws_bill_cdemo_sk          integer,\n    ws_bill_hdemo_sk          integer,\n    ws_bill_addr_sk           integer,\n    ws_ship_customer_sk       integer,\n    ws_ship_cdemo_sk          integer,\n    ws_ship_hdemo_sk          integer,\n    ws_ship_addr_sk           integer,\n    ws_web_page_sk            integer,\n    ws_web_site_sk            integer,\n    ws_ship_mode_sk           integer,\n    ws_warehouse_sk           integer,\n    ws_promo_sk               integer,\n    ws_order_number           integer,\n    ws_quantity               integer,\n    ws_wholesale_cost         decimal(7,2),\n    ws_list_price             decimal(7,2),\n    ws_sales_price            decimal(7,2),\n    ws_ext_discount_amt       decimal(7,2),\n    ws_ext_sales_price        decimal(7,2),\n    ws_ext_wholesale_cost     decimal(7,2),\n    ws_ext_list_price         decimal(7,2),\n    ws_ext_tax                decimal(7,2),\n    ws_coupon_amt             decimal(7,2),\n    ws_ext_ship_cost          decimal(7,2),\n    ws_net_paid               decimal(7,2),\n    ws_net_paid_inc_tax       decimal(7,2),\n    ws_net_paid_inc_ship      decimal(7,2),\n    ws_net_paid_inc_ship_tax  decimal(7,2),\n    ws_net_profit             decimal(7,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create catalog_sales table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.catalog_sales (\n    cs_sold_date_sk           integer,\n    cs_sold_time_sk           integer,\n    cs_ship_date_sk           integer,\n    cs_bill_customer_sk       integer,\n    cs_bill_cdemo_sk          integer,\n    cs_bill_hdemo_sk          integer,\n    cs_bill_addr_sk           integer,\n    cs_ship_customer_sk       integer,\n    cs_ship_cdemo_sk          integer,\n    cs_ship_hdemo_sk          integer,\n    cs_ship_addr_sk           integer,\n    cs_call_center_sk         integer,\n    cs_catalog_page_sk        integer,\n    cs_ship_mode_sk           integer,\n    cs_warehouse_sk           integer,\n    cs_item_sk                integer,\n    cs_promo_sk               integer,\n    cs_order_number           integer,\n    cs_quantity               integer,\n    cs_wholesale_cost         decimal(7,2),\n    cs_list_price             decimal(7,2),\n    cs_sales_price            decimal(7,2),\n    cs_ext_discount_amt       decimal(7,2),\n    cs_ext_sales_price        decimal(7,2),\n    cs_ext_wholesale_cost     decimal(7,2),\n    cs_ext_list_price         decimal(7,2),\n    cs_ext_tax                decimal(7,2),\n    cs_coupon_amt             decimal(7,2),\n    cs_ext_ship_cost          decimal(7,2),\n    cs_net_paid               decimal(7,2),\n    cs_net_paid_inc_tax       decimal(7,2),\n    cs_net_paid_inc_ship      decimal(7,2),\n    cs_net_paid_inc_ship_tax  decimal(7,2),\n    cs_net_profit             decimal(7,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\"\"\")\n\nspark.sql(f\"\"\"\n\n-- Create store_sales table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.store_sales (\n    ss_sold_date_sk           integer,\n    ss_sold_time_sk           integer,\n    ss_item_sk                integer,\n    ss_customer_sk            integer,\n    ss_cdemo_sk               integer,\n    ss_hdemo_sk               integer,\n    ss_addr_sk                integer,\n    ss_store_sk               integer,\n    ss_promo_sk               integer,\n    ss_ticket_number          integer,\n    ss_quantity               integer,\n    ss_wholesale_cost         decimal(7,2),\n    ss_list_price             decimal(7,2),\n    ss_sales_price            decimal(7,2),\n    ss_ext_discount_amt       decimal(7,2),\n    ss_ext_sales_price        decimal(7,2),\n    ss_ext_wholesale_cost     decimal(7,2),\n    ss_ext_list_price         decimal(7,2),\n    ss_ext_tax                decimal(7,2),\n    ss_coupon_amt             decimal(7,2),\n    ss_net_paid               decimal(7,2),\n    ss_net_paid_inc_tax       decimal(7,2),\n    ss_net_profit             decimal(7,2)\n) \nUSING PARQUET\nOPTIONS ('compression'='snappy');\n\n\"\"\")\n", "metadata": {"msg_id": "027f02df-c0a0-4974-b996-e7dd219d8ab4"}, "outputs": [{"execution_count": 26, "output_type": "execute_result", "data": {"text/plain": "DataFrame[]"}, "metadata": {}}], "execution_count": 26}, {"cell_type": "code", "source": "spark.sql(f\"\"\"create table if not exists {catalog_name}.{schema_name}.testTable(id INTEGER, name VARCHAR(10), age INTEGER, salary DECIMAL(10, 2)) using iceberg TBLPROPERTIES (\n    'write.parquet.compression-codec'='snappy'\n)\"\"\").show()\nspark.sql(f\"insert into {catalog_name}.{schema_name}.testTable values(1,'Alan',23,3400.00),(2,'Ben',30,5500.00),(3,'Chen',35,6500.00)\")\nspark.sql(f\"select * from {catalog_name}.{schema_name}.testTable\").show()", "metadata": {"msg_id": "cfc68804-0436-450f-9287-3e60d258bcc5"}, "outputs": [{"name": "stdout", "text": "++\n||\n++\n++\n\n+---+----+---+-------+\n| id|name|age| salary|\n+---+----+---+-------+\n|  1|Alan| 23|3400.00|\n|  2| Ben| 30|5500.00|\n|  3|Chen| 35|6500.00|\n|  1|Alan| 23|3400.00|\n|  2| Ben| 30|5500.00|\n|  3|Chen| 35|6500.00|\n+---+----+---+-------+\n\n", "output_type": "stream"}], "execution_count": 9}, {"cell_type": "code", "source": "spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.testTable.files\")", "metadata": {"msg_id": "dc36cdbe-be90-47f3-b3aa-cac9799cd9d4"}, "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "DataFrame[content: int, file_path: string, file_format: string, spec_id: int, record_count: bigint, file_size_in_bytes: bigint, column_sizes: map<int,bigint>, value_counts: map<int,bigint>, null_value_counts: map<int,bigint>, nan_value_counts: map<int,bigint>, lower_bounds: map<int,binary>, upper_bounds: map<int,binary>, key_metadata: binary, split_offsets: array<bigint>, equality_ids: array<int>, sort_order_id: int, readable_metrics: struct<age:struct<column_size:bigint,value_count:bigint,null_value_count:bigint,nan_value_count:bigint,lower_bound:int,upper_bound:int>,id:struct<column_size:bigint,value_count:bigint,null_value_count:bigint,nan_value_count:bigint,lower_bound:int,upper_bound:int>,name:struct<column_size:bigint,value_count:bigint,null_value_count:bigint,nan_value_count:bigint,lower_bound:string,upper_bound:string>,salary:struct<column_size:bigint,value_count:bigint,null_value_count:bigint,nan_value_count:bigint,lower_bound:decimal(10,2),upper_bound:decimal(10,2)>>]"}, "metadata": {}}], "execution_count": 8}, {"cell_type": "code", "source": "file_path = \"s3a://iceberg-data-bucket/demodb_snappy_v1/testTable/data/00001-6-3e3d527e-3fb9-45fc-b2ad-590d7019e1cb-0-00001.parquet\"\n\n# \ud30c\uc77c \uc77d\uae30\ndf = spark.read.format(\"parquet\").load(file_path)\n\n# Parquet \ud30c\uc77c\uc758 \uba54\ud0c0\ub370\uc774\ud130 \ucd9c\ub825\nprint(df._jdf.queryExecution().toString())\n", "metadata": {"msg_id": "a8b4345e-5175-410a-9fb6-8e1c749ba1f5"}, "outputs": [{"name": "stdout", "text": "== Parsed Logical Plan ==\nRelation [id#150,name#151,age#152,salary#153] parquet\n\n== Analyzed Logical Plan ==\nid: int, name: string, age: int, salary: decimal(10,2)\nRelation [id#150,name#151,age#152,salary#153] parquet\n\n== Optimized Logical Plan ==\nRelation [id#150,name#151,age#152,salary#153] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- FileScan parquet [id#150,name#151,age#152,salary#153] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://iceberg-data-bucket/demodb_snappy_v1/testTable/data/00001-6-3e3d..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,name:string,age:int,salary:decimal(10,2)>\n\n", "output_type": "stream"}], "execution_count": 25}, {"cell_type": "code", "source": "file_path = \"s3a://iceberg-data-bucket/demodb_snappy/testTable/data/00000-0-47a252bc-a2e1-45b2-80ef-133f4d422a44-0-00001.parquet\"\n\n# \ud30c\uc77c \uc77d\uae30\ndf = spark.read.format(\"parquet\").load(file_path)\n\n# Parquet \ud30c\uc77c\uc758 \uba54\ud0c0\ub370\uc774\ud130 \ucd9c\ub825\nprint(df._jdf.queryExecution().toString())\n", "metadata": {"msg_id": "cfed023c-3559-4aa9-a1ec-a360fff8b8bf"}, "outputs": [{"name": "stdout", "text": "== Parsed Logical Plan ==\nRelation [id#101,name#102,age#103,salary#104] parquet\n\n== Analyzed Logical Plan ==\nid: int, name: string, age: int, salary: decimal(10,2)\nRelation [id#101,name#102,age#103,salary#104] parquet\n\n== Optimized Logical Plan ==\nRelation [id#101,name#102,age#103,salary#104] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- FileScan parquet [id#101,name#102,age#103,salary#104] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://iceberg-data-bucket/demodb_snappy/testTable/data/00000-0-47a252b..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,name:string,age:int,salary:decimal(10,2)>\n\n", "output_type": "stream"}], "execution_count": 12}, {"cell_type": "code", "source": "file_path = \"s3a://iceberg-data-bucket/demodb_snappy/catalog_page/data/00000-34-fc9760c3-dab3-4602-aa7e-5ff61d69fea4-0-00001.parquet\"\n\n# \ud30c\uc77c \uc77d\uae30\ndf = spark.read.format(\"parquet\").load(file_path)\n\n# Parquet \ud30c\uc77c\uc758 \uba54\ud0c0\ub370\uc774\ud130 \ucd9c\ub825\nprint(df._jdf.queryExecution().toString())\n", "metadata": {"msg_id": "4ffa0c84-998c-442f-978d-f5c2c049e287"}, "outputs": [{"name": "stdout", "text": "== Parsed Logical Plan ==\nRelation [cp_catalog_page_sk#11,cp_catalog_page_id#12,cp_start_date_sk#13,cp_end_date_sk#14,cp_department#15,cp_catalog_number#16,cp_catalog_page_number#17,cp_description#18,cp_type#19] parquet\n\n== Analyzed Logical Plan ==\ncp_catalog_page_sk: int, cp_catalog_page_id: string, cp_start_date_sk: int, cp_end_date_sk: int, cp_department: string, cp_catalog_number: int, cp_catalog_page_number: int, cp_description: string, cp_type: string\nRelation [cp_catalog_page_sk#11,cp_catalog_page_id#12,cp_start_date_sk#13,cp_end_date_sk#14,cp_department#15,cp_catalog_number#16,cp_catalog_page_number#17,cp_description#18,cp_type#19] parquet\n\n== Optimized Logical Plan ==\nRelation [cp_catalog_page_sk#11,cp_catalog_page_id#12,cp_start_date_sk#13,cp_end_date_sk#14,cp_department#15,cp_catalog_number#16,cp_catalog_page_number#17,cp_description#18,cp_type#19] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- FileScan parquet [cp_catalog_page_sk#11,cp_catalog_page_id#12,cp_start_date_sk#13,cp_end_date_sk#14,cp_department#15,cp_catalog_number#16,cp_catalog_page_number#17,cp_description#18,cp_type#19] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://iceberg-data-bucket/demodb_snappy/catalog_page/data/00000-34-fc9..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cp_catalog_page_sk:int,cp_catalog_page_id:string,cp_start_date_sk:int,cp_end_date_sk:int,c...\n\n", "output_type": "stream"}], "execution_count": 6}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import boto3\nimport time\n\n# Create S3 client (for IBM COS)\ns3 = boto3.client(\n    's3',\n    aws_access_key_id=access_key,\n    aws_secret_access_key=secret_key,\n    endpoint_url=endpoint\n)\n\ndef human_readable_size(size_in_bytes):\n    \"\"\"\n    Convert size in bytes to a human-readable format (e.g., KB, MB, GB).\n    \"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size_in_bytes < 1024:\n            return f\"{size_in_bytes:.2f} {unit}\"\n        size_in_bytes /= 1024\n    return f\"{size_in_bytes:.2f} TB\"\n\ndef list_dat_files(bucket_name):\n    \"\"\"S3 \ubc84\ud0b7\uc5d0\uc11c .dat \ud30c\uc77c \ubaa9\ub85d\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4.\"\"\"\n    try:\n        response = s3.list_objects_v2(Bucket=bucket_name)\n        files = response.get('Contents', [])\n        dat_files = [{'Key': file['Key'], 'Size': file['Size']} for file in files if file['Key'].endswith('.dat')]\n        return dat_files\n    except Exception as e:\n        print(f\"\ud30c\uc77c \ubaa9\ub85d\uc744 \uac00\uc838\uc624\ub294 \uc911 \uc624\ub958 \ubc1c\uc0dd: {e}\")\n        return []\n\ndef drop_table_if_exists(catalog_name, schema_name, table_name):\n    \"\"\"Iceberg \ud14c\uc774\ube14\uc774 \uc874\uc7ac\ud558\uba74 \uc0ad\uc81c\ud569\ub2c8\ub2e4.\"\"\"\n    try:\n        # \ud14c\uc774\ube14 \uc874\uc7ac \uc5ec\ubd80 \ud655\uc778\n        table_exists = spark.catalog.tableExists(f\"{catalog_name}.{schema_name}.{table_name}\")\n        if table_exists:\n            # \ud14c\uc774\ube14 \uc0ad\uc81c\n            spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.{table_name}\")\n            print(f\"{table_name} \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\")\n        else:\n            print(f\"{table_name} \ud14c\uc774\ube14\uc774 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\")\n    except Exception as e:\n        print(f\"\ud14c\uc774\ube14 \uc0ad\uc81c \uc911 \uc624\ub958 \ubc1c\uc0dd: {e}\")\n\ndef get_iceberg_schema(table_name):\n    \"\"\"Iceberg \ud14c\uc774\ube14\uc758 \uc2a4\ud0a4\ub9c8\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.\"\"\"\n    try:\n        # Iceberg \ud14c\uc774\ube14\uc758 \uc2a4\ud0a4\ub9c8 \uac00\uc838\uc624\uae30\n        schema = spark.sql(f\"DESCRIBE TABLE {table_name}\").select(\"col_name\").rdd.flatMap(lambda x: x).collect()\n        # \uceec\ub7fc \uc774\ub984\ub9cc \ubc18\ud658 (\uba54\ud0c0\uc815\ubcf4 \uc81c\uc678)\n        return [col for col in schema if not col.startswith(\"#\")]\n    except Exception as e:\n        print(f\"\u274c {table_name} \ud14c\uc774\ube14\uc758 \uc2a4\ud0a4\ub9c8\ub97c \uac00\uc838\uc624\ub294 \uc911 \uc624\ub958 \ubc1c\uc0dd: {e}\")\n        return None\n\ndef cast_dataframe_to_schema(df, schema_columns, schema_dtypes):\n    \"\"\"DataFrame\uc744 Iceberg \uc2a4\ud0a4\ub9c8\uc5d0 \ub9de\uac8c \uce90\uc2a4\ud305\ud569\ub2c8\ub2e4.\"\"\"\n    from pyspark.sql.functions import col\n\n    casted_columns = []\n    for col_name, col_type in zip(schema_columns, schema_dtypes):\n        if col_name in df.columns:\n            # \ud604\uc7ac \uceec\ub7fc\uc744 Iceberg \uc2a4\ud0a4\ub9c8\uc5d0 \ub9de\ub294 \ub370\uc774\ud130 \ud0c0\uc785\uc73c\ub85c \uce90\uc2a4\ud305\n            casted_columns.append(col(col_name).cast(col_type).alias(col_name))\n        else:\n            # \ub204\ub77d\ub41c \uceec\ub7fc\uc740 null\ub85c \ucd94\uac00\n            casted_columns.append(lit(None).cast(col_type).alias(col_name))\n\n    # \uc0c8\ub85c\uc6b4 DataFrame \uc0dd\uc131\n    return df.select(*casted_columns)\n\ndef ingest_data_to_iceberg(dat_files, catalog_name, schema_name):\n    \"\"\"\uc8fc\uc5b4\uc9c4 .dat \ud30c\uc77c\ub4e4\uc744 \uc77d\uc5b4 Iceberg \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130\ub97c \uc801\uc7ac\ud569\ub2c8\ub2e4.\"\"\"\n    total_size = 0  # \ucd1d \ud30c\uc77c \ud06c\uae30 \ucd08\uae30\ud654\n    print(\"=\"*50)\n    for file_info in dat_files:\n        file = file_info['Key']\n        file_size = file_info['Size']\n        if file in ['dbgen_version']: continue\n        total_size += file_size  # \ud30c\uc77c \ud06c\uae30 \ud569\uc0b0\n        \n        # \ud30c\uc77c \uc774\ub984\uc5d0\uc11c \ud14c\uc774\ube14 \uc774\ub984 \ucd94\ucd9c\n        iceberg_table = file.split('.dat')[0]\n        full_table_name = f\"{catalog_name}.{schema_name}.{iceberg_table}\"\n\n        # \ud604\uc7ac \ud30c\uc77c \uc791\uc5c5 \uc2dc\uc791\uc744 \ud45c\uc2dc\n        print(f\"\ud604\uc7ac \uc791\uc5c5 \uc911: {file} -> \ud14c\uc774\ube14\uba85: {iceberg_table} (\ud30c\uc77c \ud06c\uae30: {human_readable_size(file_size)})\")\n        print(\"=\"*50)\n\n        # \uc791\uc5c5 \uc2dc\uc791 \uc2dc\uac04 \uae30\ub85d\n        start_time = time.time()\n\n        # Iceberg \ud14c\uc774\ube14 \uc2a4\ud0a4\ub9c8 \uac00\uc838\uc624\uae30\n        schema_df = spark.sql(f\"DESCRIBE TABLE {full_table_name}\")\n        schema_columns = schema_df.select(\"col_name\").rdd.flatMap(lambda x: x).collect()\n        schema_dtypes = schema_df.select(\"data_type\").rdd.flatMap(lambda x: x).collect()\n        \n        # \ud14c\uc774\ube14 \uc0ad\uc81c (\uc874\uc7ac\ud558\uba74)\n        # drop_table_if_exists(catalog_name, schema_name, iceberg_table)\n\n        # .dat \ud30c\uc77c \uc77d\uae30\n        df = spark.read \\\n            .format('csv') \\\n            .option('header', False) \\\n            .option('inferSchema', True) \\\n            .option('delimiter', '|') \\\n            .csv(f\"s3a://{raw_data_bucket_name}/{file}\")\n        # df.printSchema()\n        # df.show(3)\n        # df = df.toDF(*schema_columns)\n\n        # \uceec\ub7fc \uc774\ub984 \uc801\uc6a9 (Iceberg \ud14c\uc774\ube14\uc758 \uc2a4\ud0a4\ub9c8\uc5d0 \ub9de\uac8c \uc124\uc815)\n        if len(df.columns) > len(schema_columns):\n            df = df.select(df.columns[:len(schema_columns)])\n        df = df.toDF(*schema_columns)\n\n        # DataFrame\uc744 Iceberg \ud14c\uc774\ube14\uc758 \ub370\uc774\ud130 \ud0c0\uc785\uc73c\ub85c \uce90\uc2a4\ud305\n        try:\n            df = cast_dataframe_to_schema(df, schema_columns, schema_dtypes)\n        except Exception as e:\n            print(f\"\u274c \ub370\uc774\ud130 \ud0c0\uc785 \ubcc0\ud658 \uc911 \uc624\ub958 \ubc1c\uc0dd: {e}\")\n            continue\n\n        # Iceberg \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac\n        try:\n            # df.writeTo(full_table_name) \\\n            # .tableProperty(\"write.format.default\", \"parquet\") \\\n            # .tableProperty(\"write.parquet.compression-codec\", \"snappy\") \\\n            # .append()\n\n            df.writeTo(full_table_name) \\\n            .option(\"compression\", \"snappy\") \\\n            .append()\n            # df.writeTo(f\"{catalog_name}.{schema_name}.{iceberg_table}\") \\\n            #     .tableProperty(\"write.format.default\", \"parquet\") \\\n            #     .createOrReplace()\n            print(f\"\u2705 {iceberg_table} \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\")\n\n        except Exception as e:\n            print(f\"\u274c {file}\uc758 \ub370\uc774\ud130\ub97c \uc801\uc7ac\ud558\ub294 \uc911 \uc624\ub958 \ubc1c\uc0dd: {e}\")\n            continue\n        \n        # \uc791\uc5c5 \uc885\ub8cc \uc2dc\uac04 \ubc0f \uacbd\uacfc \uc2dc\uac04 \uacc4\uc0b0\n        elapsed_time = time.time() - start_time\n        print(f\"\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: {elapsed_time:.2f} \ucd08\")\n        print(\"=\"*50)\n        \n    # \ucd1d \ud30c\uc77c \ud06c\uae30 \ucd9c\ub825\n    print(f\"\\n\ud83d\udcca \uc804\uccb4 .dat \ud30c\uc77c\uc758 \ud06c\uae30 \ud569\uacc4: {human_readable_size(total_size)}\")", "metadata": {"msg_id": "f270a892-9ac5-4196-a9ba-b1ab2e79482d"}, "outputs": [], "execution_count": 40}, {"cell_type": "code", "source": "# .dat \ud30c\uc77c \ubaa9\ub85d \uac00\uc838\uc624\uae30\ndat_files = list_dat_files(raw_data_bucket_name)\ndat_files", "metadata": {"msg_id": "e03f9b41-fe47-4b3a-8f6c-8bb9c61af10f"}, "outputs": [{"execution_count": 41, "output_type": "execute_result", "data": {"text/plain": "[{'Key': 'call_center.dat', 'Size': 9356},\n {'Key': 'catalog_page.dat', 'Size': 2857922},\n {'Key': 'catalog_returns.dat', 'Size': 2279225314},\n {'Key': 'catalog_sales.dat', 'Size': 31016462258},\n {'Key': 'customer.dat', 'Size': 269515941},\n {'Key': 'customer_address.dat', 'Size': 111154196},\n {'Key': 'customer_demographics.dat', 'Size': 80660096},\n {'Key': 'date_dim.dat', 'Size': 10317438},\n {'Key': 'dbgen_version.dat', 'Size': 88},\n {'Key': 'household_demographics.dat', 'Size': 151653},\n {'Key': 'income_band.dat', 'Size': 328},\n {'Key': 'inventory.dat', 'Size': 8626269134},\n {'Key': 'item.dat', 'Size': 58366791},\n {'Key': 'promotion.dat', 'Size': 124973},\n {'Key': 'reason.dat', 'Size': 1959},\n {'Key': 'ship_mode.dat', 'Size': 1113},\n {'Key': 'store.dat', 'Size': 106820},\n {'Key': 'store_returns.dat', 'Size': 3483867155},\n {'Key': 'store_sales.dat', 'Size': 40959624908},\n {'Key': 'time_dim.dat', 'Size': 5107780},\n {'Key': 'warehouse.dat', 'Size': 1782},\n {'Key': 'web_page.dat', 'Size': 199049},\n {'Key': 'web_returns.dat', 'Size': 1053529104},\n {'Key': 'web_sales.dat', 'Size': 15463513086},\n {'Key': 'web_site.dat', 'Size': 6874}]"}, "metadata": {}}], "execution_count": 41}, {"cell_type": "code", "source": "spark.sql(f\"DROP DATABASE IF EXISTS {catalog_name}.{schema_name}\")", "metadata": {"msg_id": "f6a24e3e-7c5e-44d4-9fba-03f3766bdcca"}, "outputs": [{"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "DataFrame[]"}, "metadata": {}}], "execution_count": 12}, {"cell_type": "code", "source": "drop_table_if_exists(catalog_name, schema_name, 'testtable')", "metadata": {"msg_id": "a3d9fba0-4d12-4d91-aeb0-f14d38173ddb"}, "outputs": [{"name": "stdout", "text": "testtable \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\n", "output_type": "stream"}], "execution_count": 22}, {"cell_type": "code", "source": "dat_files = list_dat_files(raw_data_bucket_name)\nfor dat_file in dat_files:\n    tbl_name = dat_file['Key'].split('.dat')[0]\n    drop_table_if_exists(catalog_name, schema_name, tbl_name)\n    # print(tbl_name)", "metadata": {"msg_id": "09cd8939-0562-4bb1-82c3-fc6b53b8e0f9"}, "outputs": [{"name": "stdout", "text": "call_center \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ncatalog_page \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ncatalog_returns \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ncatalog_sales \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ncustomer \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ncustomer_address \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ncustomer_demographics \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ndate_dim \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ndbgen_version \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nhousehold_demographics \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nincome_band \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ninventory \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nitem \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\npromotion \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nreason \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nship_mode \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nstore \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nstore_returns \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nstore_sales \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\ntime_dim \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nwarehouse \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nweb_page \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nweb_returns \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nweb_sales \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\nweb_site \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\n", "output_type": "stream"}], "execution_count": 9}, {"cell_type": "code", "source": "# .dat \ud30c\uc77c \ubaa9\ub85d \uac00\uc838\uc624\uae30\ndat_files = list_dat_files(raw_data_bucket_name)\n\n# Iceberg \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac\ningest_data_to_iceberg(dat_files, catalog_name, schema_name)", "metadata": {"msg_id": "8ef46d7f-7bfe-44d4-835f-26f9d9d9ec6d"}, "outputs": [{"name": "stdout", "text": "==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: call_center.dat -> \ud14c\uc774\ube14\uba85: call_center (\ud30c\uc77c \ud06c\uae30: 9.14 KB)\n==================================================\n\u2705 call_center \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 20.65 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: catalog_page.dat -> \ud14c\uc774\ube14\uba85: catalog_page (\ud30c\uc77c \ud06c\uae30: 2.73 MB)\n==================================================\n\u2705 catalog_page \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 14.14 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: catalog_returns.dat -> \ud14c\uc774\ube14\uba85: catalog_returns (\ud30c\uc77c \ud06c\uae30: 2.12 GB)\n==================================================\n\u2705 catalog_returns \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 226.23 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: catalog_sales.dat -> \ud14c\uc774\ube14\uba85: catalog_sales (\ud30c\uc77c \ud06c\uae30: 28.89 GB)\n==================================================\n\u2705 catalog_sales \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 2256.28 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: customer.dat -> \ud14c\uc774\ube14\uba85: customer (\ud30c\uc77c \ud06c\uae30: 257.03 MB)\n==================================================\n\u2705 customer \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 30.38 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: customer_address.dat -> \ud14c\uc774\ube14\uba85: customer_address (\ud30c\uc77c \ud06c\uae30: 106.00 MB)\n==================================================\n\u2705 customer_address \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 23.11 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: customer_demographics.dat -> \ud14c\uc774\ube14\uba85: customer_demographics (\ud30c\uc77c \ud06c\uae30: 76.92 MB)\n==================================================\n\u2705 customer_demographics \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 16.79 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: date_dim.dat -> \ud14c\uc774\ube14\uba85: date_dim (\ud30c\uc77c \ud06c\uae30: 9.84 MB)\n==================================================\n\u2705 date_dim \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 12.37 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: dbgen_version.dat -> \ud14c\uc774\ube14\uba85: dbgen_version (\ud30c\uc77c \ud06c\uae30: 88.00 B)\n==================================================\n\u2705 dbgen_version \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 8.56 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: household_demographics.dat -> \ud14c\uc774\ube14\uba85: household_demographics (\ud30c\uc77c \ud06c\uae30: 148.10 KB)\n==================================================\n\u2705 household_demographics \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 7.70 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: income_band.dat -> \ud14c\uc774\ube14\uba85: income_band (\ud30c\uc77c \ud06c\uae30: 328.00 B)\n==================================================\n\u2705 income_band \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 7.09 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: inventory.dat -> \ud14c\uc774\ube14\uba85: inventory (\ud30c\uc77c \ud06c\uae30: 8.03 GB)\n==================================================\n\u2705 inventory \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 748.40 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: item.dat -> \ud14c\uc774\ube14\uba85: item (\ud30c\uc77c \ud06c\uae30: 55.66 MB)\n==================================================\n\u2705 item \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 14.12 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: promotion.dat -> \ud14c\uc774\ube14\uba85: promotion (\ud30c\uc77c \ud06c\uae30: 122.04 KB)\n==================================================\n\u2705 promotion \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 8.49 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: reason.dat -> \ud14c\uc774\ube14\uba85: reason (\ud30c\uc77c \ud06c\uae30: 1.91 KB)\n==================================================\n\u2705 reason \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 6.49 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: ship_mode.dat -> \ud14c\uc774\ube14\uba85: ship_mode (\ud30c\uc77c \ud06c\uae30: 1.09 KB)\n==================================================\n\u2705 ship_mode \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 6.49 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: store.dat -> \ud14c\uc774\ube14\uba85: store (\ud30c\uc77c \ud06c\uae30: 104.32 KB)\n==================================================\n\u2705 store \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 7.94 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: store_returns.dat -> \ud14c\uc774\ube14\uba85: store_returns (\ud30c\uc77c \ud06c\uae30: 3.24 GB)\n==================================================\n\u2705 store_returns \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 288.33 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: store_sales.dat -> \ud14c\uc774\ube14\uba85: store_sales (\ud30c\uc77c \ud06c\uae30: 38.15 GB)\n==================================================\n\u2705 store_sales \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 2959.21 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: time_dim.dat -> \ud14c\uc774\ube14\uba85: time_dim (\ud30c\uc77c \ud06c\uae30: 4.87 MB)\n==================================================\n\u2705 time_dim \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 11.52 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: warehouse.dat -> \ud14c\uc774\ube14\uba85: warehouse (\ud30c\uc77c \ud06c\uae30: 1.74 KB)\n==================================================\n\u2705 warehouse \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 7.01 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: web_page.dat -> \ud14c\uc774\ube14\uba85: web_page (\ud30c\uc77c \ud06c\uae30: 194.38 KB)\n==================================================\n\u2705 web_page \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 8.31 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: web_returns.dat -> \ud14c\uc774\ube14\uba85: web_returns (\ud30c\uc77c \ud06c\uae30: 1004.72 MB)\n==================================================\n\u2705 web_returns \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 141.87 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: web_sales.dat -> \ud14c\uc774\ube14\uba85: web_sales (\ud30c\uc77c \ud06c\uae30: 14.40 GB)\n==================================================\n\u2705 web_sales \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 1181.67 \ucd08\n==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: web_site.dat -> \ud14c\uc774\ube14\uba85: web_site (\ud30c\uc77c \ud06c\uae30: 6.71 KB)\n==================================================\n\u2705 web_site \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 7.77 \ucd08\n==================================================\n\n\ud83d\udcca \uc804\uccb4 .dat \ud30c\uc77c\uc758 \ud06c\uae30 \ud569\uacc4: 96.32 GB\n", "output_type": "stream"}], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "spark.sql(f\"\"\"\n\n-- Create date_dim table\nCREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.date_dim_new (\n    d_date_sk                 integer,\n    d_date_id                 varchar(16),\n    d_date                    date,\n    d_month_seq               integer,\n    d_week_seq                integer,\n    d_quarter_seq             integer,\n    d_year                    integer,\n    d_dow                     integer,\n    d_moy                     integer,\n    d_dom                     integer,\n    d_qoy                     integer,\n    d_fy_year                 integer,\n    d_fy_quarter_seq          integer,\n    d_fy_week_seq             integer,\n    d_day_name                varchar(9),\n    d_quarter_name            varchar(6),\n    d_holiday                 varchar(1),\n    d_weekend                 varchar(1),\n    d_following_holiday       varchar(1),\n    d_first_dom               integer,\n    d_last_dom                integer,\n    d_same_day_ly             integer,\n    d_same_day_lq             integer,\n    d_current_day             varchar(1),\n    d_current_week            varchar(1),\n    d_current_month           varchar(1),\n    d_current_quarter         varchar(1),\n    d_current_year            varchar(1)\n) \nUSING PARQUET\nPARTITIONED BY (d_year, d_moy)\nOPTIONS ('compression'='snappy');\"\"\")", "metadata": {"msg_id": "8062f1b0-9d6e-43ef-a33b-7b481bb19f6d"}, "outputs": [{"execution_count": 43, "output_type": "execute_result", "data": {"text/plain": "DataFrame[]"}, "metadata": {}}], "execution_count": 43}, {"cell_type": "code", "source": "dat_files = list_dat_files(raw_data_bucket_name)\ndat_files[7:8]", "metadata": {"msg_id": "08c550be-7d63-41ae-bc1a-50f3eddeb9bf"}, "outputs": [{"execution_count": 21, "output_type": "execute_result", "data": {"text/plain": "[{'Key': 'date_dim.dat', 'Size': 10317438}]"}, "metadata": {}}], "execution_count": 21}, {"cell_type": "code", "source": "drop_table_if_exists(catalog_name, schema_name, 'date_dim_new')", "metadata": {"msg_id": "1fa6c189-fd0c-4dd5-ad57-9109b8f7bc5d"}, "outputs": [{"name": "stdout", "text": "date_dim_new \ud14c\uc774\ube14 \uc0ad\uc81c \uc644\ub8cc\n", "output_type": "stream"}], "execution_count": 42}, {"cell_type": "code", "source": "ingest_data_to_iceberg(dat_files[7:8], catalog_name, schema_name)", "metadata": {"msg_id": "3f961568-5d4a-422b-8d8a-94df4065e6e4"}, "outputs": [{"name": "stdout", "text": "==================================================\n\ud604\uc7ac \uc791\uc5c5 \uc911: date_dim.dat -> \ud14c\uc774\ube14\uba85: date_dim (\ud30c\uc77c \ud06c\uae30: 9.84 MB)\n==================================================\n\u2705 date_dim \ud14c\uc774\ube14\uc5d0 \ub370\uc774\ud130 \uc801\uc7ac \uc644\ub8cc\n\uc791\uc5c5 \uc644\ub8cc \uc2dc\uac04: 16.87 \ucd08\n==================================================\n\n\ud83d\udcca \uc804\uccb4 .dat \ud30c\uc77c\uc758 \ud06c\uae30 \ud569\uacc4: 9.84 MB\n", "output_type": "stream"}], "execution_count": 25}, {"cell_type": "code", "source": "spark.sql(f\"\"\"\nINSERT INTO {catalog_name}.{schema_name}.date_dim_new\nSELECT * FROM {catalog_name}.{schema_name}.date_dim;\n\"\"\")\n", "metadata": {"msg_id": "ba89a73c-2693-49fa-8ea5-1a429e988492"}, "outputs": [{"execution_count": 29, "output_type": "execute_result", "data": {"text/plain": "DataFrame[]"}, "metadata": {}}], "execution_count": 29}, {"cell_type": "code", "source": "import time\nstart=time.time()\nelapsed_time = time.time() - start\n\n# \uacbd\uacfc \uc2dc\uac04 \ucd9c\ub825\nprint(f\"Elapsed time: {elapsed_time:.2f} seconds\")", "metadata": {"msg_id": "30c37c47-339c-476e-a1ba-22e3994a4f88"}, "outputs": [{"name": "stdout", "text": "Elapsed time: 0.00 seconds\n", "output_type": "stream"}], "execution_count": 30}, {"cell_type": "code", "source": "import time\n\ndf = spark.table(f\"{catalog_name}.{schema_name}.date_dim\")\n\nstart=time.time()\n# \ub370\uc774\ud130\ub97c \uc0c8\ub85c\uc6b4 \ud30c\ud2f0\uc158\uc5d0 \uc4f0\uae30\ndf.repartition(\"d_year\", \"d_moy\") \\\n  .write \\\n  .format(\"parquet\") \\\n  .mode(\"append\") \\\n  .partitionBy(\"d_year\", \"d_moy\") \\\n  .saveAsTable(f\"{catalog_name}.{schema_name}.date_dim_new\")\n# \uacbd\uacfc \uc2dc\uac04 \ucd9c\ub825\nelapsed_time = time.time() - start\nprint(f\"Elapsed time: {elapsed_time:.2f} seconds\")", "metadata": {"msg_id": "f2bbc000-f247-49df-92a3-5190251bccb2"}, "outputs": [{"name": "stdout", "text": "Elapsed time: 388.64 seconds\n", "output_type": "stream"}], "execution_count": null}, {"cell_type": "code", "source": "import time\n\ndf = spark.table(f\"{catalog_name}.{schema_name}.date_dim\")\n\nstart=time.time()\n# \ub370\uc774\ud130\ub97c \uc0c8\ub85c\uc6b4 \ud30c\ud2f0\uc158\uc5d0 \uc4f0\uae30\ndf.repartition(\"d_year\", \"d_moy\") \\\n  .write \\\n  .format(\"parquet\") \\\n  .mode(\"append\") \\\n  .partitionBy(\"d_year\", \"d_moy\") \\\n  .saveAsTable(f\"{catalog_name}.{schema_name}.date_dim_new\")\n# \uacbd\uacfc \uc2dc\uac04 \ucd9c\ub825\nelapsed_time = time.time() - start\nprint(f\"Elapsed time: {elapsed_time:.2f} seconds\")", "metadata": {"msg_id": "31c9f939-a8ab-45e8-a7ea-0dfac13465a6"}, "outputs": [{"name": "stdout", "text": "Elapsed time: 493.15 seconds\n", "output_type": "stream"}], "execution_count": 33}, {"cell_type": "code", "source": "for key, value in spark.sparkContext.getConf().getAll():\n    print(f\"{key} = {value}\")", "metadata": {"msg_id": "b8ff19f1-eea8-4eba-9003-6552cfd598f1"}, "outputs": [{"name": "stdout", "text": "spark.hive.metastore.truststore.password = changeit\nspark.hive.metastore.truststore.type = JKS\nspark.eventLog.enabled = true\nspark.network.crypto.keyLength = 256\nspark.driver.memory = 3600M\nspark.network.crypto.enabled = true\nspark.driver.extraClassPath = /home/spark/user_home/dbdrivers/*:/cc-home/_global_/dbdrivers/*:/cc-home/_global_/dbdrivers/jdbc/default/*:/home/spark/shared/user-libs/spark/*:/home/spark/shared/user-libs/common/*:/home/spark/shared/user-libs/connectors/*:/home/spark/space/assets/data_asset/*:/project_data/data_asset/*:/opt/ibm/spark/external-jars/iceberg-spark-runtime-iceberg-spark.jar:/opt/ibm/connectors/parquet-encryption/*:/opt/ibm/third-party/libs/spark2/*:/opt/ibm/third-party/libs/common/*:/opt/ibm/third-party/libs/connectors/*:/opt/ibm/connectors/others-db-drivers/*:/opt/ibm/connectors/wdp-connector-driver/*:/opt/ibm/connectors/db2/*:/opt/ibm/connectors/data-engine/*:/opt/ibm/connectors/wxd/*:/opt/ibm/connectors/snowflake/*:/opt/ibm/spark/external-jars/*\nspark.hive.metastore.truststore.path = file:///opt/ibm/jdk/lib/security/cacerts\nspark.authenticate.enableSaslEncryption = true\nspark.ui.killEnabled = false\nspark.app.id = app-20241225021546-0001\nspark.serializer = org.apache.spark.serializer.KryoSerializer\nspark.r.command = /home/spark/shared/R/bin/Rscript\nspark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false  -Dderby.system.home=/.local/share/jupyter/runtime/kernel-b301a6b3-3f01-47b1-b3df-7dfca696c3c5-20241225_013228 -Dlog4j.logFile=/home/spark/shared/logs/kernel-python3.10-python3.10-20241225_013228.log -Dlog4j.configuration=file:/opt/ibm/jkg/log4j/log4j.properties -Dsemeru.fips=false -Dlog4j.configurationFile=file:/opt/ibm/ae-config/log4j2.properties --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -Djdk.nativeDigest=false -Dfile.encoding=UTF-8 \nspark.hadoop.fs.stocator.glob.bracket.support = true\nspark.local.dir = /tmp/spark/scratch\nspark.hive.metastore.client.plain.username = ibmlhapikey_cpadmin\nspark.sql.parquet.compression.codec = snappy\nspark.ui.reverseProxy = false\nspark.executor.memory = 3600M\nspark.hadoop.fs.s3a.bucket.raw-data-bucket-300gb.aws.credentials.provider = com.ibm.iae.s3.credentialprovider.WatsonxCredentialsProvider\nspark.hadoop.fs.s3a.bucket.iceberg-data-bucket.custom.signers = WatsonxAWSV4Signer:com.ibm.iae.s3.credentialprovider.WatsonxAWSV4Signer\nspark.hadoop.fs.s3a.bucket.raw-data-bucket-300gb.endpoint = s3.jp-tok.cloud-object-storage.appdomain.cloud\nspark.authenticate.secret = 527aa411-bc48-4d51-a19c-55e54579f2f7\nspark.blockManager.port = 6068\nspark.hadoop.fs.stocator.scheme.list = cos\nspark.sql.catalog.hive.warehouse = s3a://raw-data-bucket-300gb\nspark.sql.catalog.iceberg_data_catalog.warehouse = s3a://iceberg-data-bucket\nspark.driver.cores = 2\nspark.hive.metastore.uris = thrift://ibm-lh-lakehouse-mds-thrift-svc.cpd-operands.svc.cluster.local:8380\nspark.ui.enabled = true\nspark.hadoop.wxd.apikey = ZenApiKey Y3BhZG1pbjphVzV6ZEdGdVkyVTZPbkZyTWprMk1tOXJkelZzT1RjMWQyUT0=\nspark.executor.id = driver\nspark.hadoop.fs.s3a.bucket.raw-data-bucket.custom.signers = WatsonxAWSV4Signer:com.ibm.iae.s3.credentialprovider.WatsonxAWSV4Signer\nspark.driver.port = 6067\nspark.wxd.api.endpoint = https://lhconsole-api-svc.cpd-operands.svc.cluster.local:3333\nspark.hadoop.wxd.instanceId = 1734602430095944\nspark.sql.warehouse.dir = file:/home/spark/shared/spark-warehouse\nspark.hive.metastore.client.plain.password = aW5zdGFuY2U6OnFrMjk2Mm9rdzVsOTc1d2Q=\nspark.hadoop.fs.cos.impl = com.ibm.stocator.fs.ObjectStoreFileSystem\nspark.hadoop.fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.wxd.cas.endpoint = https://ibm-lh-lakehouse-cas-svc.cpd-operands.svc.cluster.local:8780/cas/v1/signature\nspark.hadoop.fs.s3a.bucket.raw-data-bucket-300gb.custom.signers = WatsonxAWSV4Signer:com.ibm.iae.s3.credentialprovider.WatsonxAWSV4Signer\nspark.sql.catalog.iceberg = org.apache.iceberg.spark.SparkCatalog\nspark.sql.catalogImplementation = hive\nspark.master.ui.port = 8080\nspark.executor.instances = 1\nspark.master = spark://spark-master-headless-b301a6b3-3f01-47b1-b3df-7dfca696c3c5:7077\nspark.history.fs.logDirectory = /home/spark/shared/spark-event\nspark.app.submitTime = 1735090355166\nspark.sql.catalog.hive.type = hive\nspark.hadoop.fs.s3a.fast.upload = true\nspark.authenticate = true\nspark.dynamicAllocation.enabled = false\nspark.app.startTime = 1735092946066\nspark.hive.metastore.client.auth.mode = PLAIN\nspark.sql.catalog.iceberg_data_catalog.type = hive\nspark.shuffle.service.enabled = false\nspark.hadoop.fs.s3a.bucket.raw-data-bucket-300gb.s3.signing-algorithm = WatsonxAWSV4Signer\nspark.eventLog.dir = file:///home/spark/spark-events\nspark.ui.https.enabled = true\nspark.hadoop.hive.wxd.user.name = cpadmin\nspark.hadoop.fs.s3a.bucket.raw-data-bucket.aws.credentials.provider = com.ibm.iae.s3.credentialprovider.WatsonxCredentialsProvider\nspark.ui.port = 4040\nspark.driver.host = spark-master-headless-b301a6b3-3f01-47b1-b3df-7dfca696c3c5\nspark.hadoop.fs.s3a.bucket.raw-data-bucket.secret.key = 423c929fb85ff48809011f3264e10e7a87240150a5896b01\nspark.sql.catalog.iceberg_data_catalog = org.apache.iceberg.spark.SparkCatalog\nspark.hadoop.fs.s3a.bucket.iceberg-data-bucket.endpoint = s3.jp-tok.cloud-object-storage.appdomain.cloud\nspark.serializer.objectStreamReset = 100\nspark.hive.metastore.use.SSL = true\nspark.hadoop.fs.s3a.bucket.raw-data-bucket.access.key = 9d05516331d44f38ba2c230ff036e8a0\nspark.submit.deployMode = client\nspark.executor.extraClassPath = /home/spark/user_home/dbdrivers/*:/cc-home/_global_/dbdrivers/*:/cc-home/_global_/dbdrivers/jdbc/default/*:/home/spark/shared/user-libs/spark/*:/home/spark/shared/user-libs/common/*:/home/spark/shared/user-libs/connectors/*:/home/spark/space/assets/data_asset/*:/project_data/data_asset/*:/opt/ibm/spark/external-jars/iceberg-spark-runtime-iceberg-spark.jar:/opt/ibm/connectors/parquet-encryption/*:/opt/ibm/third-party/libs/spark2/*:/opt/ibm/third-party/libs/common/*:/opt/ibm/third-party/libs/connectors/*:/opt/ibm/connectors/others-db-drivers/*:/opt/ibm/connectors/wdp-connector-driver/*:/opt/ibm/connectors/db2/*:/opt/ibm/connectors/data-engine/*:/opt/ibm/connectors/wxd/*:/opt/ibm/connectors/snowflake/*:/opt/ibm/spark/external-jars/*\nspark.app.name = python3.10\nspark.sql.catalog.iceberg.warehouse = s3a://raw-data-bucket\nspark.hadoop.fs.s3a.bucket.iceberg-data-bucket.aws.credentials.provider = com.ibm.iae.s3.credentialprovider.WatsonxCredentialsProvider\nspark.history.ui.port = 18080\nspark.worker.ui.port = 8081\nspark.hadoop.fs.s3a.bucket.raw-data-bucket.endpoint = https://s3.jp-tok.cloud-object-storage.appdomain.cloud\nspark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension,org.apache.spark.sql.hudi.HoodieSparkSessionExtension\nspark.shuffle.service.port = 7337\nspark.executor.cores = 2\nspark.hadoop.fs.s3a.bucket.iceberg-data-bucket.s3.signing-algorithm = WatsonxAWSV4Signer\nspark.driver.bindAddress = 0.0.0.0\nspark.rdd.compress = True\nspark.hadoop.fs.stocator.cos.impl = com.ibm.stocator.fs.cos.COSAPIClient\nspark.ssl.ui.port = 4440\nspark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false  -Dderby.system.home=/.local/share/jupyter/runtime/kernel-b301a6b3-3f01-47b1-b3df-7dfca696c3c5-20241225_013228 -Dlog4j.logFile=/home/spark/shared/logs/kernel-python3.10-python3.10-20241225_013228.log -Dlog4j.configuration=file:/opt/ibm/jkg/log4j/log4j.properties -Dsemeru.fips=false -Dlog4j.configurationFile=file:/opt/ibm/ae-config/log4j2.properties -Divy.cache.dir=/tmp -Divy.home=/tmp -Djdk.nativeDigest=false -Dfile.encoding=UTF-8 \nspark.submit.pyFiles = \nspark.hadoop.fs.s3a.bucket.raw-data-bucket.s3.signing-algorithm = WatsonxAWSV4Signer\nspark.hadoop.fs.s3a.multipart.size = 33554432\nspark.sql.catalog.iceberg.type = hive\nspark.hadoop.fs.stocator.cos.scheme = cos\nspark.ui.showConsoleProgress = true\nspark.sql.iceberg.vectorization.enabled = false\n", "output_type": "stream"}], "execution_count": 34}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}]}